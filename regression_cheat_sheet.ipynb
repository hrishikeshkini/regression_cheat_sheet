{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample statistical chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "# We can override the default matplotlib styles with those of Seaborn\n",
    "#import seaborn as sns\n",
    "#sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter plot (first we put the horizontal axis, then the vertical axis)\n",
    "plt.scatter(x,y)\n",
    "# Name the axes\n",
    "plt.xlabel('x-axis', fontsize = 20)\n",
    "plt.ylabel('y-axis', fontsize = 20)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant. Essentially, we are adding a new column (equal in lenght to x), which consists only of 1s\n",
    "x1 = sm.add_constant(x)\n",
    "# Fit the model, according to the OLS (ordinary least squares) method with a dependent variable y and an idependent x\n",
    "results = sm.OLS(y,x1).fit()\n",
    "# Print a nice summary of the regression. That's one of the strong points of statsmodels -> the summaries\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the R-squared in sklearn we must call the appropriate method\n",
    "reg.score(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the coefficients of the regression\n",
    "# Note that the output is an array, as we usually expect several coefficients\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intercept (Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the intercept of the regression\n",
    "# Note that the result is a float as we usually expect a single value\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different ways to plot the data - here's the matplotlib code\n",
    "plt.scatter(x,y)\n",
    "\n",
    "# Parametrized version of the regression line\n",
    "yhat = reg.coef_*x + reg.intercept_\n",
    "\n",
    "# Non-parametrized version of the regression line\n",
    "#yhat = 0.0017*x + 0.275\n",
    "\n",
    "# Plotting the regression line\n",
    "fig = plt.plot(x,yhat, lw=4, c='orange', label ='regression line')\n",
    "\n",
    "# Labelling our axes\n",
    "plt.xlabel('SAT', fontsize = 20)\n",
    "plt.ylabel('GPA', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula for Adjusted R^2\n",
    "\n",
    "$R^2_{adj.} = 1 - (1-R^2)*\\frac{n-1}{n-p-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to find the Adjusted R-squared we can do so by knowing the r2, the # observations, the # features\n",
    "r2 = reg.score(x,y)\n",
    "# Number of observations is the shape along axis 0\n",
    "n = x.shape[0]\n",
    "# Number of features (predictors, p) is the shape along axis 1\n",
    "p = x.shape[1]\n",
    "\n",
    "# We find the Adjusted R-squared using the formula\n",
    "adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R^2 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different ways to solve this problem\n",
    "# To make it as easy and interpretable as possible, we have preserved the original code\n",
    "def adj_r2(x,y):\n",
    "    r2 = reg.score(x,y)\n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the result\n",
    "adj_r2(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "### Calculate the univariate p-values of the variables\n",
    "Full documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the feature selection module from sklearn\n",
    "# This module allows us to select the most appopriate features for our regression\n",
    "# There exist many different approaches to feature selection, however, we will use one of the simplest\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look into: f_regression\n",
    "# f_regression finds the F-statistics for the *simple* regressions created with each of the independent variables\n",
    "# In our case, this would mean running a simple linear regression on GPA where SAT is the independent variable\n",
    "# and a simple linear regression on GPA where Rand 1,2,3 is the indepdent variable\n",
    "# The limitation of this approach is that it does not take into account the mutual effect of the two features\n",
    "f_regression(x,y)\n",
    "\n",
    "# There are two output arrays\n",
    "# The first one contains the F-statistics for each of the regressions\n",
    "# The second one contains the p-values of these F-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are more interested in the latter (p-values), we can just take the second array\n",
    "p_values = f_regression(x,y)[1]\n",
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to quickly evaluate them, we can round the result to 3 digits after the dot\n",
    "p_values.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to (properly) include p-values in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the p-values are obtained through certain statistics, we need the 'stat' module from scipy.stats\n",
    "import scipy.stats as stat\n",
    "\n",
    "# Since we are using an object oriented language such as Python, we can simply define our own \n",
    "# LinearRegression class (the same one from sklearn)\n",
    "# By typing the code below we will ovewrite a part of the class with one that includes p-values\n",
    "# Here's the full source code of the ORIGINAL class: https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/linear_model/base.py#L362\n",
    "\n",
    "\n",
    "class LinearRegression(linear_model.LinearRegression):\n",
    "    \"\"\"\n",
    "    LinearRegression class after sklearn's, but calculate t-statistics\n",
    "    and p-values for model coefficients (betas).\n",
    "    Additional attributes available after .fit()\n",
    "    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n",
    "    which is (n_features, n_coefs)\n",
    "    This class sets the intercept to 0 by default, since usually we include it\n",
    "    in X.\n",
    "    \"\"\"\n",
    "    \n",
    "    # nothing changes in __init__\n",
    "    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n",
    "                 n_jobs=1):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.copy_X = copy_X\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, n_jobs=1):\n",
    "        self = super(LinearRegression, self).fit(X, y, n_jobs)\n",
    "        \n",
    "        # Calculate SSE (sum of squared errors)\n",
    "        # and SE (standard error)\n",
    "        sse = np.sum((self.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n",
    "\n",
    "        # compute the t-statistic for each feature\n",
    "        self.t = self.coef_ / se\n",
    "        # find the p-value for each feature\n",
    "        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we create the regression everything is the same\n",
    "reg_with_pvalues = LinearRegression()\n",
    "reg_with_pvalues.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_with_pvalues.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new data frame with the names of the features\n",
    "reg_summary = pd.DataFrame([['SAT'],['Rand 1,2,3']],columns =['Features'])\n",
    "# Then we create and fill a second column, called 'Coefficients' with the coefficients of the regression\n",
    "reg_summary['Coefficients'] = reg_with_pvalues.coef_\n",
    "# Finally, we add the p-values we just calculated\n",
    "reg_summary['p-values'] = reg_with_pvalues.p.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new data frame with the names of the features\n",
    "reg_summary = pd.DataFrame(data = x.columns.values, columns=['Features'])\n",
    "reg_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create and fill a second column, called 'Coefficients' with the coefficients of the regression\n",
    "reg_summary ['Coefficients'] = reg.coef_\n",
    "# Finally, we add the p-values we just calculated\n",
    "reg_summary ['p-values'] = p_values.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got a pretty clean summary, which can help us make an informed decision about the inclusion of the variables \n",
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the OLS assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we decided to use some matplotlib code, without explaining it\n",
    "# You can simply use plt.scatter() for each of them (with your current knowledge)\n",
    "# But since Price is the 'y' axis of all the plots, it made sense to plot them side-by-side (so we can compare them)\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'Price' as y\n",
    "ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])\n",
    "ax1.set_title('Price and Year')\n",
    "ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])\n",
    "ax2.set_title('Price and EngineV')\n",
    "ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])\n",
    "ax3.set_title('Price and Mileage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly see the columns of our data frame\n",
    "data_cleaned.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn does not have a built-in way to check for multicollinearity\n",
    "# one of the main reasons is that this is an issue well covered in statistical frameworks and not in ML ones\n",
    "# surely it is an issue nonetheless, thus we will try to deal with it\n",
    "\n",
    "# Here's the relevant module\n",
    "# full documentation: http://www.statsmodels.org/dev/_modules/statsmodels/stats/outliers_influence.html#variance_inflation_factor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# To make this as easy as possible to use, we declare a variable where we put\n",
    "# all features where we want to check for multicollinearity\n",
    "# since our categorical data is not yet preprocessed, we will only take the numerical ones\n",
    "variables = data_cleaned[['Mileage','Year','EngineV']]\n",
    "\n",
    "# we create a new data frame which will include all the VIFs\n",
    "# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \n",
    "vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "# Finally, I like to include names so it is easier to explore the result\n",
    "vif[\"Features\"] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the result\n",
    "# keep only values which are in range 3 to 9\n",
    "vif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
